import requestsimport timeimport refrom lxml import etreefrom fake_useragent import UserAgentimport syssys.path.append("..")from utils.web_request import WebRequestua = UserAgent(use_cache_server=False)def get_html_tree(url, **kwargs):    """    获取html树    :param url:    :param kwargs:    :return:    """    header = {        'Connection': 'keep-alive',        'Cache-Control': 'max-age=0',        'Upgrade-Insecure-Requests': '1',        'User-Agent': ua.random,        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',        'Accept-Encoding': 'gzip, deflate, sdch',        'Accept-Language': 'zh-CN,zh;q=0.8',    }    # TODO 取代理服务器用代理服务器访问    wr = WebRequest()    # delay 2s for per request    time.sleep(2)    html = wr.get(url=url, header=header).content    return etree.HTML(html)def is_useful_proxy(proxy):    """    Checking is proxy valid.    :param proxy:    :return: boolean    """    if isinstance(proxy, bytes):        proxy = proxy.decode('utf8')    proxies = {"http": "http://{proxy}".format(proxy=proxy)}    # noinspection PyBroadException,PyUnusedLocal    try:        # Not using the proxy timeout over 20s.        r = requests.get('http://httpbin.org/ip', proxies=proxies, timeout=20,                         verify=False)        if r.status_code == 200:            return True    except Exception as e:        return Falsedef get_data5u_proxy():    """    proxy address: http://www.data5u.com/    :return:    """    url_list = [        'http://www.data5u.com/',        'http://www.data5u.com/free/index.shtml',        'http://www.data5u.com/free/gngn/index.shtml',        'http://www.data5u.com/free/gnpt/index.shtml'    ]    for url in url_list:        html_tree = get_html_tree(url)        ul_list = html_tree.xpath('//ul[@class="l2"]')        for ul in ul_list:            # noinspection PyBroadException,PyUnusedLocal            try:                yield ':'.join(ul.xpath('.//li/text()')[0:2])            except Exception as e:                passdef get_66ip_proxy(proxy_number=100):    """    proxy address: http://www.66ip.cn/    :param proxy_number: proxy_number    :return:    """    url = "http://www.66ip.cn/mo.php?sxb=&tqsl={}&port=&export=&ktip=&sxa=&submit=%CC%E1++%C8%A1&textarea=".format(        proxy_number)    response = WebRequest()    html = response.get(url).text    for proxy in re.findall(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d{1,5}',                            html):        yield proxydef get_ip181_proxy():    """    proxy address: http://www.ip181.com/    :return:    """    url = 'http://www.ip181.com/'    html_tree = get_html_tree(url)    # noinspection PyBroadException,PyUnusedLocal    try:        tr_list = html_tree.xpath('//tr')[1:]        for tr in tr_list:            yield ':'.join(tr.xpath('./td/text()')[0:2])    except Exception as e:        passdef get_xicidaili_proxy():    """    proxy address: http://www.xicidaili.com/    :return:    """    url_list = [        'http://www.xicidaili.com/nn',        'http://www.xicidaili.com/nt',    ]    for each_url in url_list:        tree = get_html_tree(each_url)        proxy_list = tree.xpath('.//table[@id="ip_list"]//tr')        for proxy in proxy_list:            # noinspection PyBroadException,PyUnusedLocal            try:                yield ':'.join(proxy.xpath('./td/text()')[0:2])            except Exception as e:                passdef get_guobanjia_proxy():    """    proxy address: http://www.goubanjia.com/free/gngn/index.shtml    :return:    """    url = "http://www.goubanjia.com/free/gngn/index{page}.shtml"    for page in range(1, 10):        page_url = url.format(page=page)        tree = get_html_tree(page_url)        proxy_list = tree.xpath('//td[@class="ip"]')        xpath_str = """.//*[not(contains(@style, 'display: none'))                                    and not(contains(@style, 'display:none'))                                    and not(contains(@class, 'port'))                                    ]/text()                    """        for each_proxy in proxy_list:            # noinspection PyBroadException,PyUnusedLocal            try:                ip_addr = ''.join(each_proxy.xpath(xpath_str))                port = each_proxy.xpath(                    ".//span[contains(@class, 'port')]/text()")[0]                yield '{}:{}'.format(ip_addr, port)            except Exception as e:                passdef get_xdaili_proxy():    """    proxy address: http://www.xdaili.cn/ipagent/freeip/getFreeIps?page=1&rows=10    :return:    """    url = 'http://www.xdaili.cn/ipagent/freeip/getFreeIps?page=1&rows=10'    request = WebRequest()    # noinspection PyBroadException,PyUnusedLocal    try:        res = request.get(url).json()        for row in res['RESULT']['rows']:            yield '{}:{}'.format(row['ip'], row['port'])    except Exception as e:        passdef get_kuaidaili_proxy():    """    proxy address: https://www.kuaidaili.com    """    url = 'https://www.kuaidaili.com/free/inha/{page}/'    for page in range(1, 10):        page_url = url.format(page=page)        tree = get_html_tree(page_url)        proxy_list = tree.xpath('.//table//tr')        for tr in proxy_list[1:]:            yield ':'.join(tr.xpath('./td/text()')[0:2])def get_yundaili_proxy():    """    proxy address: http://www.yun-daili.com/    :return:    """    url = 'http://www.yun-daili.com/free.asp?stype=1&page={page}'    for page in range(1, 10):        page_url = url.format(page=page)        tree = get_html_tree(page_url)        proxy_list = tree.xpath('.//table//tr')        for tr in proxy_list[1:20]:            yield ':'.join(tr.xpath('./td/text()')[0:2])class FreeProxy(object):    """    :param proxy    :param [kwargs]    """    def __init__(self, proxy, **kwargs):        # Check params is callable.        assert hasattr(proxy, '__call__')        self.proxy = proxy(kwargs) if kwargs else proxy()        from collections import Iterable        # Check params is iterable.        assert isinstance(self.proxy, Iterable)    def get_free_proxy(self):        return self.proxyif __name__ == '__main__':    fp = FreeProxy(get_yundaili_proxy).get_free_proxy()    for i in fp:        print(i)